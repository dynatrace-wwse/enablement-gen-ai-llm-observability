{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"1. About","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"#whats-this-tutorial-all-about","title":"What's this tutorial all about","text":"<p>In this tutorial we'll learn the basics about AI &amp; LLM Monitoring.</p> <p>Dynatrace supports AI and LLM Observability for more than 40 different technologies providing visibility into the different layers of AI and LLM applications.</p> <ul> <li>Monitor service health and performance: Track real-time metrics (request counts, durations, and error rates). Stay aligned with SLOs.</li> <li>Monitor service quality and cost: Implement error budgets for performance and cost control. Validate model consumption and response times. Prevent quality degradation by monitoring models and usage patterns in real time.</li> <li>End-to-end tracing and debugging: Trace prompt flows from initial request to final response for quick root cause analysis and troubleshoot. Gain granular visibility into LLM prompt latencies and model-level metrics. Pinpoint issues in prompts, tokens, or system integrations.</li> <li>Build trust, reduce compliance and audit risks: Track every input and output for an audit trail. Query all data in real time and store for future reference. Maintain full data lineage from prompt to response.</li> </ul> <p>What will we do</p> <p>In this tutorial we will learn how it is easy to observe an AI application that uses Ollama as Large Language Model, Weaviate as Vector Database, and LangChain as an orchestrator to create Retrieval augmented generation (RAG) and Agentic AI Pipelines.</p> <ul> <li>Yes! let's begin </li> </ul>"},{"location":"2-getting-started/","title":"2. Getting started","text":"<p>Requirements</p> <ul> <li>A Grail enabled Dynatrace SaaS Tenant (sign up here).</li> <li>A GitHub account to interact with the demo repository.</li> </ul>"},{"location":"2-getting-started/#prerequisites-before-launching-the-codespace","title":"Prerequisites before launching the Codespace","text":""},{"location":"2-getting-started/#generate-a-dynatrace-token","title":"Generate a Dynatrace Token","text":"<p>To create a Dynatrace token</p> <ol> <li>In Dynatrace, go to Access Tokens.     To find Access Tokens, press Ctrl/Cmd+K to search for and select Access Tokens.</li> <li>In Access Tokens, select Generate new token.</li> <li>Enter a Token name for your new token.</li> <li>Give your new token the following permissions:</li> <li>Search for and select all of the following scopes.<ul> <li>Ingest metrics (<code>metrics.ingest</code>)</li> <li>Ingest logs (<code>logs.ingest</code>)</li> <li>Ingest events (<code>events.ingest</code>)</li> <li>Ingest OpenTelemetry traces (<code>openTelemetryTrace.ingest</code>)</li> <li>Read metrics (<code>metrics.read</code>)</li> <li>Write settings (<code>settings.write</code>)</li> </ul> </li> <li>Select Generate token.</li> <li>Copy the generated token to the clipboard and be ready to use it in the next step.</li> </ol> <p>You can only access your token once upon creation. You can't reveal it afterward.</p> <p>Let's launch the Codespace</p> <p>Now we are ready to launch the Codespace!</p> <ul> <li>Let's launch Codespaces</li> </ul>"},{"location":"3-codespaces/","title":"3. Codespaces","text":"<p>This codespace is powered by the Dynatrace Enablement Framework, this means that this codespace:</p> <ul> <li>can run in github codespaces, as a remote container or locally as docker container</li> <li>is crosscompiled for AMD and ARM architectures</li> <li>follows a set of standards and best practices for enhancing the user experience</li> </ul> <p>Want to learn more about it? We invite you to read this documentation</p> <p></p>"},{"location":"3-codespaces/#11-codespaces-configuration","title":"1.1 Codespaces configuration","text":"<p>Branch, Machine sizing &amp; secrets</p> <ul> <li>Branch<ul> <li>select the main branch</li> </ul> </li> <li>Machine sizing<ul> <li>As a machine type select 4-core</li> </ul> </li> <li>Secrets (enter your credentials within the following variables)<ul> <li>DT_TENANT</li> <li>DT_TOKEN</li> </ul> </li> </ul>"},{"location":"3-codespaces/#2-while-the-codespace-is-set-up-for-you-learn-powerful-usecases-with-dynatrace","title":"2. While the Codespace is set-up for you, learn powerful usecases with Dynatrace","text":"<p>We know your time is very valuable. This codespace takes around 6 minutes to be fully operational. A local Kubernetes (kind) cluster monitored by Dynatrace will be configured and in it a sample AI application, the AI Travel Advisor app will be deployed. To make your experience best, we are also installing and configuring tools like:</p> <p>k9s kubectl helm node jq python3 gh</p> <p></p>"},{"location":"3-codespaces/#3-explore-what-has-been-deployed","title":"3. Explore what has been deployed","text":"<p>Your Codespace has now deployed the following resources:</p> <ul> <li> <p>A local Kubernetes (kind) cluster monitored by Dynatrace, with some pre-deployed apps   that will be used later in the demo.</p> </li> <li> <p>After a couple of minutes, you'll see this screen in your codespaces terminal. It contains the links to the local expose labguide and the UI of the application which we will be doing our Hands-On training.</p> </li> </ul> <p></p>"},{"location":"3-codespaces/#4-tips-tricks","title":"4. Tips &amp; Tricks","text":"<p>We want to boost your learning and try to make your DEV experience as smooth as possible with Dynatrace trainings. Your Codespaces have a couple of convenience features added. </p>"},{"location":"3-codespaces/#show-the-greeting","title":"Show the greeting","text":"<p>In the terminal, there are functions loaded for your convenience. By creating a new Terminal the Greeting will be shown that includes the links to the exposed apps, the Github  pages, the Github Repository, the Dynatrace Tenant that is bound to this devcontainer and some of the tools installed.</p> <p>You can create a new Terminal directly in VSCode, type <code>zsh</code> or call the function <code>printGreeting</code> and that will print the greeting with the most relevant information.</p>"},{"location":"3-codespaces/#navigating-in-your-local-kubernetes","title":"Navigating in your local Kubernetes","text":"<p>The client <code>kubectl</code> and <code>k9s</code>are configured so you can navigate in your local Kubernetes like butter.  </p>"},{"location":"3-codespaces/#exposing-the-app-to-the-public","title":"Exposing the app to the public","text":"<p>The app AI Travel Advisor is being exposed in the devcontainer to your localhost via Nodeport. If you want to make the endpoints public accesible, just go to the ports section in VsCode, right click on them and change the visibility to public.</p>"},{"location":"3-codespaces/#5-troubleshooting","title":"5. Troubleshooting","text":""},{"location":"3-codespaces/#exposing-the-app","title":"Exposing the App","text":"<p>The AI Travel Advisor app is being exposed via NodePort to the Kubernetes Workernode port 30100. You can easily see what is being exposed by typing the function <code>showOpenPorts</code> </p> <pre><code>showOpenPorts(){\n  sudo netstat -tulnp\n}\n</code></pre> <ul> <li>Let's start our AI journey</li> </ul>"},{"location":"4-content/","title":"Content","text":""},{"location":"4-content/#what-is-ai-and-llm-observability","title":"What is AI and LLM observability?","text":"<p>AI and Large Language Model (LLM) observability provides visibility into all layers AI-powered applications. This approach covers the complete AI stack, from foundational models and vector databases to RAG orchestration frameworks, ensuring visibility across every layer of modern AI applications.  Complete observability is critical to ensure accuracy and reliability.</p>"},{"location":"4-content/#the-need-for-ai-and-llm-observability","title":"The need for AI and LLM observability","text":"<p>As large language models have evolved, many use cases have emerged. Common AI implementations include chatbots, data analysis, data extraction, code creation, and content creation. These AI-powered models offer benefits such as speed, scope, and scale. LLMs can quickly handle complex queries using a variety of data types from multiple data sources.</p> <p>However, synthesizing more data faster doesn't always mean better results. Models may function perfectly, but if the data sources aren't accurate, outputs will be inaccurate, as well. Furthermore, if the data is valid, but processes are flawed, results won't be reliable. Therefore, observability is necessary to ensure that all aspects of the LLM operations are correct and consistent.</p>"},{"location":"4-content/#key-components-of-ai-and-llm-observability","title":"Key components of AI and LLM observability","text":"<p>AI observability has three key components:</p>"},{"location":"4-content/#1-output-evaluation","title":"1.- Output evaluation","text":"<p>Teams must regularly evaluate outputs for accuracy and reliability. Because many organizations use third-party models, teams often accomplish this using a separate evaluation LLM that\u2019s purpose-built for this function.</p>"},{"location":"4-content/#2-prompt-analysis","title":"2.- Prompt analysis","text":"<p>Poorly constructed prompts are a common cause of low-quality results. Therefore, LLM observability regularly analyzes prompts to determine if queries produce desired results and if better prompt templates can improve them.</p>"},{"location":"4-content/#3-retrieval-improvement","title":"3.- Retrieval improvement","text":"<p>Data search and retrieval are critical for effective output. Here, the observability solution considers the retrieved data's context and accuracy, and it looks for ways to improve this process.</p>"},{"location":"4-content/#lets-see-an-example-of-an-ai-application-the-ai-travel-advisor","title":"Let's see an example of an AI Application, the AI Travel Advisor","text":"<p>In VSCode open a new terminal and in the Welcome Message you should see a link to the AI Travel Advisor App UI.  Click on it, you should see something like this:</p> <p></p> <p>The application is a basic implementation of AI technologies where the user can type the name of a city and then the AI will give back a travel advice depending on the selected Mode enhancing the user input like the following: User input converted into travel advice<pre><code> Question: Give travel advise in a paragraph of max 50 words about {input}  \n</code></pre></p> <p>This sample application is monitored with OpenLLMetry, an open source library built on top of OpenTelemetry to provide auto-instrumentation for AI Technologies. To configure OpenLLMetry, you should add the following code snippet before any library is imported. It is common practice to add it to the first lines of the main python script. The initialization requires a Dynatrace Token and a OTLP endpoint. To find the available OTLP endpoints supported by Dynatrace, please refer to our documentation.</p> OpenLLMetry setup<pre><code>from traceloop.sdk import Traceloop\n# Dynatrace API Auth Header\nheaders = {\"Authorization\": f\"Api-Token {TOKEN}\"}\n# Initialize OpenLLMetry\nTraceloop.init(\n    app_name=\"ai-travel-advisor\",\n    api_endpoint=OTEL_ENDPOINT,\n    disable_batch=True,\n    headers=headers,\n)\n</code></pre> <p>OpenLLMetry has to be initialized before the import of the AI technologies because it proxies the import of the libraries and changes their code to provide additional telemetry. If the library is already imported, the Python interpreter won't see the changes introduced by OpenLLMetry.</p> <p>The AI Travel Advisor App has three different modes to interact with an LLM:</p> <ul> <li>Direct Chat</li> <li>RAG</li> <li>Agentic</li> </ul> <p>Let's explore them!</p> <ul> <li>Let's interact with the AI</li> </ul>"},{"location":"5-direct/","title":"Direct Chat","text":"<p>In this mode, we directly send our prompt to Ollama to generate a travel raccomendation. Let's use <code>Sydney</code> as city to test the AI Travel raccomendation and press <code>Advise</code>. At first, you will see a loading animation and after a few seconds, an answer like the following one:</p> <p></p> <p>If you click on Advise multiple time, you will see that the answer is slightly different every time. LLMs have inherently a random nature which makes Observability important.</p> <p>We saw that the response arrived after a few seconds, how can we sure that the response time is not sky rocketing?  How can be sure that the LLM answers correctly to user requests?  If we're using 3rd party LLMs, how can we monitor the cost associated with the requests?</p> <p>Luckily, we have Dynatrace \ud83d\ude09</p>"},{"location":"5-direct/#lets-follow-the-traces","title":"Let's follow the traces","text":"<p>Opening the Distributed Tracing App, you can filter it by services on the menu on the left-hand side.  You should see the service <code>ai-travel-advisor</code>.  Filtering by the service name allows to see all the request that we made so far to our AI Application. Clicking on one, will reveal the set of operation that are happening when we request a travel advice.</p> <p></p> <p>The request is quite simple, we hit the API exposed by the service, we start a Task which denotes a specific operation or step within a workflow. Finally, we fire a call towards Ollama to generate a completion for the prompt.</p> <p>If OneAgent is also being deployed, we will see addional child traces that represent the call received by Ollama and the processing of the request.</p> <p>From this view, we have an understanding of what's happening and how long it took the service to provide an answer to the user.  But let's navigate a bit deeper on what information are at our disposal!</p> <p>Clicking on the span <code>ollama.completion</code> we get more insights into what's happening when we call our LLMs.  In particular, the section around GenAI and LLM contain the relevant information for the domain, such as token consumption, prompt details, and model used. Based on which LLM the application is using, we can get more details, such as temperature, topK, and more deep level type of information.</p> <p></p> <p>However, directly chatting with a model is not how moder AI application approach problems. These LLMs are powerful but don't know about a specific domains and contexes. In the next section, we're exploring a common approach to overcome this limitation.</p> <ul> <li>Let's create an advance AI pipeline</li> </ul>"},{"location":"6-rag/","title":"Retrieval-Augmented Generation (RAG)","text":"<p>Retrieval-Augmented Generation (RAG) is a technique that provides additional context to LLMs to have additional information to carry out a solution to the prompt they receive. In our AI Travel Advisor App, the RAG pipeline is built using LangChain. The retrieval step reaches out to Weaviate to query for documents relevant to the prompt in input.</p> <p></p> <p>Let's try again using the same city as input: <code>Sydney</code>. The answer returned should be quite puzzling, like the following:</p> <p></p> <p>In this answer, the LLM is hallucinating an answer which is far from being correct! Let's investigate what could be the reason of such a weird answer.</p>"},{"location":"6-rag/#tracing-to-the-rescue","title":"Tracing to the rescue","text":"<p>Let's use again the Distributed Tracing App to inspect the request.</p> <p></p> <p>We can see that the request is more complex because there is a step to fetch documents from Weaviate, process them, augment the prompt and finally send the final crafted prompt to Ollama. Fetching the documents is a fast step that only takes few milliseconds and crafting the final response is taking almost all the time. Selecting each span, we have at our disposal all the contextual information that describe that AI pipeline step.</p> <p>Let's focus on the time expensive call to Ollama and select the <code>ChatOllama.chat</code> span. In the detailed view, we can see the GenAI section. Let's start from the prompt message:</p> <p></p> <p>We can see that the prompt sent to the LLM contains information about Sydney and Bali. Clearly something is wrong! LangChain retrieves the top N documents closest to the topic searched.  Let's see the fetched documents by pressing on the <code>retriever.done.task</code> span and looking into its attributes.</p> <p></p> <p>We see that the query is looking for <code>Sydney</code> and two documents have been retrieved from Weaviate, one for Sydney and one for Bali. If we look into the application code, inside the destinations folder, we see only two small documents which contain innacurate information.</p> <p>The lack of coverage of the topic triggered the fetching of additional documents that don't really relate to Sydney. This is a clear indicator that our Knowledge Base inside Weaviate is not exahustive enough.</p> <p>Furthermore, we can also observe that feeding the LLM with garbage information produces garbage responses. The content of the Sydney or Bali documents provide innacurate facts.  This is telling us that the LLM really made use of the information we provided to it.</p> <p>Let's try again using a different city, like <code>Paris</code>. Since we don't have wrong information around Paris, now the LLM should produce a valid answer!</p> <p></p> <p>There is a bug with OpenLLMetry and Weaviate for which it does generate Spans only for v3 of Weaviate. In this codebase, we fix it by explicity telling the sensor what function it should attach to it.</p> <ul> <li>Let's explore another way of using AI</li> </ul>"},{"location":"7-agentic/","title":"Agentic AI","text":"<p>Agentic AI is a novel technique where AI systems act independently and proactively, aiming to achieve specific goals without direct human intervention. Integrating these systems into digital services poses challenges due to their non-deterministic communication, which can create unpredictable behavior. This makes observability essential to monitor and control their interactions. As the number of AI and cloud-native service instances grows, managing these autonomous systems becomes more complex. Observability serves as a crucial feedback channel to orchestrate and moderate the outcomes of Agentic AI, ensuring effective communication among the agents.</p> <p>In our AI Travel Advisor app, we have three different agents:</p> <ul> <li>Valid City: verifies that a prompt is asking about a valid city name</li> <li>Travel Advice: provides a travel reccomendation</li> <li>Excuse: provides an excuse why the AI chat-bot cannot answer the request</li> </ul> <p>These Agents are orchestrated through LangChain using a complex system prompt. The orchestrator is the componenet that moves the control- and data-flow between agents.</p> <p>In this codespace, we're running a small Ollama model. Hence, most of the time the request will terminate with an error or a timeout limit is reached. Changing the LLM from Ollama to a foundation one, like a Bedrock model, ChatGPT, or Google Gemini will solve these issues.</p> <p></p>"},{"location":"7-agentic/#lets-try-to-understand-whats-happening","title":"Let's try to understand what's happening","text":"<p>Why do we get an error? Let's analyze the situation with our handy-dandy Distributed Tracing App! </p> <p>In the trace, we can see how many more spans are created. This shows the complexity of Agentic AI and why observability is crucial.</p> <p></p> <p>In Agentic AI, an LLM decides what's the next Agent to call and with what data.  Tracing becomes crucial to explain why the AI arrived at that conclusion and we can do that by following the Chain of Thoughts (CoT). For this, we can click over a <code>ollama.chat</code> span to have access to the CoT. Analyzing the span details, we can see in the prompts the system prompts used.</p> <p></p> <p>The first system prompt contains the tools, expressed in JSON format, that the LLM has access to and the instructions to answering following the format that LangChain is expecting the AI to answer. The second system prompt contains the task that the AI has to solve, verify if the city is correct and provide a travel advice.</p> <p>We can then see in the completion attributes that the LLM didn't follow correctly the format expected. Moving to the next span <code>JSONAgentOutputParser</code>, we can see that an exception was recorded.  Clicking on the respective tab, we can see that the format used by Ollama doesn't follow JSON making it impossible to parse the response.</p> <p></p> <p>We can analyze the rest of the trace to check that te orchestrator tried 3 more times before admiting defeat and provide a standard error message back to us.</p>"},{"location":"7-agentic/#how-does-it-look-like-when-we-use-a-more-powerful-model","title":"How does it look like when we use a more powerful model?","text":"<p>Great question! Let's see the same example if we use Google Gemini!</p> <p></p> <p>In the trace, we can see that we first call the <code>valid_city.tool</code> and afterwards, the <code>travel_advice.tool</code>. Gemini is smart enought to understand the correct sequence of actions. Furthermore, if we look at the span details, we can see the CoT follow the correct format with the Thought used by the AI and what action should be called. For the keen eye, we can also see in the trace a last <code>VertexAI.completion</code> span after the <code>travel_advice.tool</code>. This is because LangChain asks the AI if we reached the end and we solved the task or if we should continue solving it.</p> <p></p> <p>Analyzing the CoT of this step, we can see that Gemini correctly marked the sequence of reasoning with <code>Final Answer</code> so the orchestrator knows that it can return the response to the user.</p> <ul> <li>Cleanup</li> </ul>"},{"location":"cleanup/","title":"8. Cleanup","text":"<p>Deleting the codespace from inside the container</p> <p>We like to make your life easier, for convenience there is a function loaded in the shell of the Codespace for deleting the codespace, just type <code>deleteCodespace</code>. This will trigger the deletion of the codespace.</p> <p>Another way to do this is by going to https://github.com/codespaces and delete the codespace.</p> <p>You may also want to deactivate or delete the API token needed for this lab.</p> <ul> <li>Ressources</li> </ul>"},{"location":"resources/","title":"9. Resources","text":""},{"location":"resources/#get-your-dynatrace-environment","title":"Get your Dynatrace environment","text":"<ul> <li>Create a Free Trial in Dynatrace</li> </ul>"},{"location":"resources/#documentation","title":"Documentation","text":"<ul> <li>Dynatrace documentation - AI and LLM Observability</li> <li>Dynatrace AI Observability Solution</li> <li>Technology supported</li> </ul>"},{"location":"resources/#dynatrace-news","title":"Dynatrace news","text":"<ul> <li>Deliver secure, safe, and trustworthy GenAI applications with Amazon Bedrock and Dynatrace</li> <li>Dynatrace accelerates business transformation with new AI observability solution</li> <li>Why 85% of AI projects fail and how Dynatrace can save yours</li> <li>Dynatrace Blog</li> </ul>"},{"location":"resources/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Highspot - How to sell AI Observability</li> <li>SE Demo Pitch Deck</li> <li>Customer Pitch Deck</li> <li>App/Ready-Made dashboards that can be installed to all DPS/Saas instances</li> </ul> <ul> <li>What's Next? </li> </ul>"},{"location":"whats-next/","title":"10. What's next?","text":"<p>More to come</p> <ul> <li>Stay tuned, more enablements are coming whith more AI technologies, models, etc.</li> </ul> <p>In this example, we only touched the surface of AI and LLM Observability. Different AI Vendors have different key signals and additional enterprise features.</p> <p>For example, Bedorock supports Guardrails - a mechanism to protect AI system from Toxic language, PII leaks or talking about forbiden topics, or OpenAI provides Prompt Caching to save on cost and performance.</p> <p>To showcase the additional information, we built several dashboard that are shipped with the AI Observability app.</p> <p>The app is available in the Playground Tenant and provides examples for:</p> <ul> <li>Amazon Bedrock</li> <li>Azure AI Foundry</li> <li>Google Vertex and Gemini</li> <li>OpenAI</li> </ul> <p></p>"},{"location":"snippets/admonitions/","title":"Admonitions","text":"<p>Warning</p> <p>This is a Warning </p> <p>Note</p> <p>This is a Note </p> <p>Important</p> <p>This is important </p> <p>Tipp</p> <p>This is a tipp </p>"},{"location":"snippets/disclaimer/","title":"Disclaimer","text":"<p>Support Policy</p> <p>This is an enablement project created by the Center of Excellence - Enablement Team at Dynatrace.</p> <p>Support is provided via GitHub issues only. The materials provided in this repository are offered \"as-is\" without any warranties, express or implied. Use them at your own risk.</p>"},{"location":"snippets/dt-enablement/","title":"Dt enablement","text":"<p>This codespace is powered by the Dynatrace Enablement Framework, this means that this codespace:</p> <ul> <li>can run in github codespaces, as a remote container or locally as docker container</li> <li>is crosscompiled for AMD and ARM architectures</li> <li>follows a set of standards and best practices for enhancing the user experience</li> </ul> <p>Want to learn more about it? We invite you to read this documentation</p>"},{"location":"snippets/grail-requirements/","title":"Grail requirements","text":"<p>Requirements</p> <ul> <li>A Grail enabled Dynatrace SaaS Tenant (sign up here).</li> <li>A GitHub account to interact with the demo repository.</li> </ul>"},{"location":"snippets/view-code/","title":"View code","text":"<p>View the Code</p> <p>The code for this repository is hosted on GitHub. Click the \"View Code on GitHub\" link above.</p>"}]}